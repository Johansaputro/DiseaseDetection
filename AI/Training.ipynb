{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f61843a0-6d1b-4848-b84a-74a54184e1f3",
   "metadata": {},
   "source": [
    "## Prepare Dataset List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67faf7a5-8a0c-4ca4-8a77-89d501cf88ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset as dataset\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import scipy.ndimage as ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c4f1428-1b08-4f52-b16c-7e0010843d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 30\n",
    "lower = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7fc5e993-47f2-483b-be39-f7addf130f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(dataset):\n",
    "    def  __init__ (self , ct_dir, seg_dir):\n",
    "        self.ct_list = os.listdir(ct_dir)\n",
    "        self.seg_list = list(map(lambda x: x.replace('img', 'label'), self.ct_list))\n",
    "\n",
    "        self.ct_list = list(map(lambda x: os.path.normpath(os.path.join(ct_dir, x)), self.ct_list))\n",
    "        self.seg_list = list(map(lambda x: os.path.normpath(os.path.join(seg_dir, x)), self.seg_list))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        :param index:\n",
    "        :return: torch.Size([B, 1, 48, 256, 256]) torch.Size([B, 48, 256, 256])\n",
    "        \"\"\"\n",
    "        \n",
    "        ct_path = self.ct_list[index]\n",
    "        seg_path  = self.seg_list[index]\n",
    "\n",
    "        # Read CT and gold standard into memory\n",
    "        ct  =  sitk.ReadImage(ct_path , sitk.sitkInt16)\n",
    "        seg  =  sitk.ReadImage(seg_path , sitk.sitkUInt8)\n",
    "\n",
    "        ct_array = sitk.GetArrayFromImage(ct)\n",
    "        seg_array = sitk.GetArrayFromImage(seg)\n",
    "\n",
    "        #Randomly select 48 slices in the slice plane\n",
    "        start_slice = random.randint(0, ct_array.shape[0] - size)\n",
    "        end_slice = start_slice + size - 1\n",
    "\n",
    "        ct_array = ct_array[start_slice:end_slice + 1, :, :]\n",
    "        seg_array = seg_array [start_slice:end_slice + 1 , :, :]\n",
    "\n",
    "#         # Randomly rotate within 5 degrees with probability 0.5\n",
    "#         # If the angle is negative, it will rotate clockwise, if the angle is positive, it will rotate counterclockwise\n",
    "        if random.uniform(0, 1) >= 0.5:\n",
    "            angle = random.uniform(-5, 5)\n",
    "            ct_array = ndimage.rotate(ct_array, angle, axes =(1, 2), reshape = False , cval = lower);\n",
    "            seg_array = ndimage.rotate(seg_array, angle, axes = (1, 2), reshape = False, cval = 0);\n",
    "\n",
    "        #There is a probability of 0.5 without any modification, and the remaining 0.5 randomly selects a patch with a size of 0.8-0.5 and enlarges it to 256*256\n",
    "#         if random.uniform(0, 1) >= 0.5:\n",
    "#             ct_array, seg_array = self.zoom(ct_array, seg_array, patch_size=random.uniform(0.5, 0.8))\n",
    "        \n",
    "#         ct_array = F.interpolate(ct_array, (size, 256, 256), mode='trilinear').squeeze().detach().numpy()\n",
    "#         seg_array = F.interpolate(seg_array, (size, 256, 256), mode='trilinear').squeeze().detach().numpy()\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             ct_array = torch.FloatTensor(ct_array).unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "#             ct_array = Variable(ct_array)\n",
    "#             ct_array = F.interpolate(ct_array, (size, 256, 256), mode='trilinear').squeeze().detach().numpy()\n",
    "\n",
    "#             seg_array = torch.FloatTensor(seg_array).unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "#             seg_array = Variable(seg_array)\n",
    "#             seg_array = F.interpolate(seg_array, (size, 256, 256), mode='trilinear').squeeze().detach().numpy()\n",
    "        \n",
    "\n",
    "        # After processing, convert array to tensor\n",
    "        ct_array = torch.FloatTensor(ct_array).unsqueeze(0)\n",
    "        seg_array = torch.FloatTensor(seg_array)\n",
    "        \n",
    "        ct_array = F.interpolate(ct_array, (size, 256, 256), mode='trilinear').squeeze().detach().numpy()\n",
    "        seg_array = F.interpolate(seg_array, (size, 256, 256), mode='trilinear').squeeze().detach().numpy()\n",
    "        \n",
    "        # ct_array = torch.FloatTensor(ct_array)\n",
    "        # seg_array = torch.FloatTensor(seg_array)\n",
    "        \n",
    "        print(\"{} {}\".format(ct_array.shape, seg_array.shape))\n",
    "\n",
    "        return ct_array, seg_array\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ct_list)\n",
    "\n",
    "    def zoom(self, ct_array, seg_array, patch_size):\n",
    "\n",
    "        length = int(256*patch_size)\n",
    "\n",
    "        x1 = int(random.uniform(0, 255-length))\n",
    "        y1 = int(random.uniform(0, 255-length))\n",
    "\n",
    "        x2 = x1 + length\n",
    "        y2 = y1 + length\n",
    "\n",
    "        ct_array = ct_array[:, x1:x2 + 1, y1:y2 + 1]\n",
    "        seg_array = seg_array[:, x1:x2 + 1 , y1:y2 + 1]    \n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            ct_array = torch.FloatTensor(ct_array).unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "            ct_array = Variable(ct_array)\n",
    "            ct_array = F.interpolate(ct_array, (size, 256, 256), mode='trilinear').squeeze().detach().numpy()\n",
    "\n",
    "            seg_array = torch.FloatTensor(seg_array).unsqueeze(dim=0).unsqueeze(dim=0)\n",
    "            seg_array = Variable(seg_array)\n",
    "            seg_array = F.interpolate(seg_array, (size, 256, 256), mode='trilinear').squeeze().detach().numpy()\n",
    "\n",
    "            return ct_array, seg_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2435a2d4-1cfa-4ba4-980b-dfa76299bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_dir = \"E:/skripsi/Dataset/train/CT\"\n",
    "seg_dir = \"E:/skripsi/Dataset/train/GT\"\n",
    "\n",
    "train_ds = Dataset(ct_dir, seg_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f5ac7b7d-31a2-43b9-8490-e90c3696d0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-0.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-1.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-10.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-12.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-13.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-14.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-15.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-17.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-18.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-19.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-20.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-21.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-22.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-23.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-25.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-26.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-27.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-28.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-3.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-30.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-31.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-32.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-34.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-35.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-36.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-37.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-38.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-4.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-40.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-41.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-42.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-44.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-45.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-46.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-48.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-49.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-5.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-51.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-52.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-53.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-54.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-55.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-56.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-57.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-59.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-6.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-60.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-8.nii',\n",
       " 'E:\\\\skripsi\\\\Dataset\\\\train\\\\GT\\\\label-9.nii']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.seg_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2886335f-0292-4cac-a26e-354454fd75ee",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "aa90c806-b8ab-480a-9099-ddefbee9526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "812fba88-248f-4ff2-aced-0052d5ddb66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(skip_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, 1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        g = self.W_g(skip)\n",
    "        x = self.W_x(x)\n",
    "\n",
    "        psi = self.relu(g + x)\n",
    "        psi = self.psi(psi)\n",
    "\n",
    "        return x * psi\n",
    "\n",
    "class UpConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpConv, self).__init__()\n",
    "        self.up_conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.up_conv(x)\n",
    "\n",
    "class AttU_Net(nn.Module):\n",
    "    def __init__(self, in_channels=2, out_channels=2, init_features=32):\n",
    "        super(AttU_Net, self).__init__()\n",
    "\n",
    "        self.down1 = DoubleConv(in_channels, init_features)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.down2 = DoubleConv(init_features, init_features * 2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.down3 = DoubleConv(init_features * 2, init_features * 4)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.down4 = DoubleConv(init_features * 4, init_features * 8)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.center = DoubleConv(init_features * 8, init_features * 16)\n",
    "\n",
    "        self.up4 = UpConv(init_features * 16, init_features * 8)\n",
    "        self.att4 = AttentionBlock(init_features * 8, init_features * 4, init_features * 8)\n",
    "        self.up3 = UpConv(init_features * 8, init_features * 4)\n",
    "        self.att3 = AttentionBlock(init_features * 4, init_features * 2, init_features * 4)\n",
    "        self.up2 = UpConv(init_features * 4, init_features * 2)\n",
    "        self.att2 = AttentionBlock(init_features * 2, init_features * 2, init_features * 2)\n",
    "        self.up1 = UpConv(init_features * 2, init_features)\n",
    "        self.att1 = AttentionBlock(init_features, init_features, init_features)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(init_features, out_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        skip1 = self.down1(x)\n",
    "        pool1 = self.pool1(skip1)\n",
    "\n",
    "        skip2 = self.down2(pool1)\n",
    "        pool2 = self.pool2(skip2)\n",
    "\n",
    "        skip3 = self.down3(pool2)\n",
    "        pool3 = self.pool3(skip3)\n",
    "\n",
    "        skip4 = self.down4(pool3)\n",
    "        pool4 = self.pool4(skip4)\n",
    "\n",
    "        center = self.center(pool4)\n",
    "\n",
    "        up4 = self.up4(center)\n",
    "        att4 = self.att4(up4, skip4)\n",
    "\n",
    "        up3 = self.up3(att4)\n",
    "        att3 = self.att3(up3, skip3)\n",
    "\n",
    "        up2 = self.up2(att3)\n",
    "        att2 = self.att2(up2, skip2)\n",
    "\n",
    "        up1 = self.up1(att2)\n",
    "        att1 = self.att1(up1, skip1)\n",
    "\n",
    "        out = self.final_conv(att1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb8126e-38fc-4ee0-a84d-402fe602b7db",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "bdcd144e-eea0-4680-8a3c-604addf92fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "30b141ed-4b73-4179-9040-053ea04bed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0ff6f0c6-6a18-469d-b9bd-ba94351826b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cdf1939f-0390-43ff-9828-ff87f41c5cc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input and output must have the same number of spatial dimensions, but got input with with spatial dimensions of [160, 160] and output size of (30, 256, 256). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[140], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     19\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     21\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m     22\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mE:\\anaconda\\envs\\FlaskAi\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    650\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    651\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 652\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    656\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mE:\\anaconda\\envs\\FlaskAi\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    691\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 692\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    694\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mE:\\anaconda\\envs\\FlaskAi\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mE:\\anaconda\\envs\\FlaskAi\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[132], line 60\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     57\u001b[0m ct_array \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(ct_array)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     58\u001b[0m seg_array \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(seg_array)\n\u001b[1;32m---> 60\u001b[0m ct_array \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mct_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrilinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     61\u001b[0m seg_array \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(seg_array, (size, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrilinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# ct_array = torch.FloatTensor(ct_array)\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# seg_array = torch.FloatTensor(seg_array)\u001b[39;00m\n",
      "File \u001b[1;32mE:\\anaconda\\envs\\FlaskAi\\lib\\site-packages\\torch\\nn\\functional.py:3854\u001b[0m, in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[0;32m   3852\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(size, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m   3853\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m-> 3854\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3855\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput and output must have the same number of spatial dimensions, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3856\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput with with spatial dimensions of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and output size of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3857\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide input tensor in (N, C, d1, d2, ...,dK) format and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3858\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput size in (o1, o2, ...,oK) format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3859\u001b[0m \n\u001b[0;32m   3860\u001b[0m         )\n\u001b[0;32m   3861\u001b[0m     output_size \u001b[38;5;241m=\u001b[39m size\n\u001b[0;32m   3862\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Input and output must have the same number of spatial dimensions, but got input with with spatial dimensions of [160, 160] and output size of (30, 256, 256). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format."
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "num_epochs = 50\n",
    "batch_size = 4\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Define loss function and device\n",
    "loss_fn = nn.CrossEntropyLoss() # use cross-entropy loss for multi-class segmentation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define dataset and dataloader\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define model and optimizer\n",
    "model = AttU_Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fa054d-61da-41b0-8e87-208488012ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cf3efd-b462-4f93-8e08-1abb73d34ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed95eba9-fd24-480a-b918-c84e506080d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FlaskAi",
   "language": "python",
   "name": "flaskai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
